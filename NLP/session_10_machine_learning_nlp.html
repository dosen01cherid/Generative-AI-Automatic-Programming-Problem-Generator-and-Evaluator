<!DOCTYPE html>
<html lang="id">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Session 10: Machine Learning untuk NLP</title>

    <!-- MathJax -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Plotly -->
    <script src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>

    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: #333;
            overflow-x: hidden;
        }

        .slide {
            min-height: 100vh;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            padding: 60px 40px;
            background: white;
            margin: 20px auto;
            max-width: 1200px;
            border-radius: 20px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
        }

        .slide:first-child {
            margin-top: 40px;
        }

        .slide:last-child {
            margin-bottom: 40px;
        }

        h1 {
            font-size: 3em;
            color: #667eea;
            margin-bottom: 20px;
            text-align: center;
            font-weight: 700;
        }

        h2 {
            font-size: 2.5em;
            color: #764ba2;
            margin-bottom: 30px;
            text-align: center;
            font-weight: 600;
        }

        h3 {
            font-size: 1.8em;
            color: #667eea;
            margin: 30px 0 15px 0;
            font-weight: 600;
        }

        h4 {
            font-size: 1.4em;
            color: #764ba2;
            margin: 20px 0 10px 0;
            font-weight: 500;
        }

        p, li {
            font-size: 1.2em;
            line-height: 1.8;
            margin-bottom: 15px;
            color: #555;
        }

        .subtitle {
            font-size: 1.5em;
            color: #666;
            text-align: center;
            margin-bottom: 40px;
        }

        .content-box {
            background: #f8f9fa;
            border-left: 5px solid #667eea;
            padding: 25px;
            margin: 20px 0;
            border-radius: 10px;
            width: 100%;
        }

        .example-box {
            background: #fff3cd;
            border-left: 5px solid #ffc107;
            padding: 25px;
            margin: 20px 0;
            border-radius: 10px;
        }

        .formula-box {
            background: #e3f2fd;
            border: 2px solid #2196F3;
            padding: 25px;
            margin: 25px 0;
            border-radius: 10px;
            text-align: center;
        }

        .important-box {
            background: #ffebee;
            border-left: 5px solid #f44336;
            padding: 25px;
            margin: 20px 0;
            border-radius: 10px;
        }

        .success-box {
            background: #e8f5e9;
            border-left: 5px solid #4caf50;
            padding: 25px;
            margin: 20px 0;
            border-radius: 10px;
        }

        ul, ol {
            margin-left: 40px;
            margin-bottom: 20px;
        }

        li {
            margin-bottom: 10px;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 25px 0;
            font-size: 1.1em;
        }

        th, td {
            padding: 15px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }

        th {
            background-color: #667eea;
            color: white;
            font-weight: 600;
        }

        tr:hover {
            background-color: #f5f5f5;
        }

        .code-block {
            background: #2d2d2d;
            color: #f8f8f2;
            padding: 20px;
            border-radius: 10px;
            font-family: 'Courier New', monospace;
            font-size: 1em;
            overflow-x: auto;
            margin: 20px 0;
        }

        .highlight {
            background: #ffeb3b;
            padding: 2px 6px;
            border-radius: 3px;
            font-weight: 600;
        }

        .badge {
            display: inline-block;
            padding: 8px 16px;
            background: #667eea;
            color: white;
            border-radius: 20px;
            font-size: 0.9em;
            margin: 5px;
        }

        .plot-container {
            width: 100%;
            height: 500px;
            margin: 30px 0;
        }

        .grid-2 {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 30px;
            margin: 30px 0;
        }

        .grid-3 {
            display: grid;
            grid-template-columns: 1fr 1fr 1fr;
            gap: 20px;
            margin: 30px 0;
        }

        @media (max-width: 768px) {
            .grid-2, .grid-3 {
                grid-template-columns: 1fr;
            }

            h1 {
                font-size: 2em;
            }

            h2 {
                font-size: 1.8em;
            }
        }

        .footer {
            text-align: center;
            color: #999;
            font-size: 1em;
            margin-top: 40px;
        }
    </style>
</head>
<body>

<!-- Slide 1: Title -->
<div class="slide">
    <h1>Session 10</h1>
    <h2>Machine Learning untuk NLP</h2>
    <p class="subtitle">Text Classification, Naive Bayes, Logistic Regression, SVM</p>
    <div style="margin-top: 40px;">
        <span class="badge">Natural Language Processing</span>
        <span class="badge">Semester 6</span>
        <span class="badge">Week 10</span>
    </div>
    <div class="footer">
        Universitas Mercu Buana<br>
        Program Studi Teknik Informatika
    </div>
</div>

<!-- Slide 2: Big Problems ML Solves in NLP -->
<div class="slide">
    <h2>üéØ Mengapa Machine Learning untuk NLP?</h2>
    <div class="important-box">
        <h3>Masalah Besar yang Diselesaikan:</h3>
        <p><strong>1. Ambiguitas dan Kompleksitas Bahasa Natural</strong></p>
        <ul>
            <li>Bahasa manusia penuh dengan ambiguitas, konteks, dan nuansa</li>
            <li>Rule-based systems terlalu rapuh dan tidak bisa menangkap semua variasi</li>
            <li><strong>ML Solution:</strong> Belajar pola dari data, bukan hardcode rules</li>
        </ul>
    </div>

    <div class="important-box">
        <p><strong>2. Skalabilitas dan Generalisasi</strong></p>
        <ul>
            <li>Impossible untuk menulis rules untuk setiap kasus di dunia nyata</li>
            <li>Bahasa terus berkembang dengan kata dan ekspresi baru</li>
            <li><strong>ML Solution:</strong> Model belajar representasi yang dapat digeneralisasi ke data baru</li>
        </ul>
    </div>

    <div class="important-box">
        <p><strong>3. Otomasi Tugas yang Membutuhkan Pemahaman Semantik</strong></p>
        <ul>
            <li>Classification, sentiment analysis, topic modeling memerlukan pemahaman meaning</li>
            <li>Manual labeling tidak praktis untuk millions of documents</li>
            <li><strong>ML Solution:</strong> Automated classification dengan akurasi tinggi</li>
        </ul>
    </div>
</div>

<!-- Slide 3: Learning Objectives -->
<div class="slide">
    <h2>üìö Capaian Pembelajaran</h2>
    <div class="content-box">
        <h3>Sub-CPMK Week 10:</h3>
        <p>Mahasiswa mampu menerapkan algoritma machine learning klasik (Naive Bayes, Logistic Regression, SVM) untuk tugas klasifikasi teks dan sentiment analysis</p>
    </div>
    <div class="grid-2">
        <div class="content-box">
            <h4>‚úÖ Kemampuan yang Dikuasai:</h4>
            <ul>
                <li>Memahami supervised learning untuk NLP</li>
                <li>Implementasi Naive Bayes classifier</li>
                <li>Implementasi Logistic Regression</li>
                <li>Memahami Support Vector Machines</li>
                <li>Feature engineering untuk text</li>
                <li>Evaluasi model classification</li>
            </ul>
        </div>
        <div class="content-box">
            <h4>üìã Topik Pembahasan:</h4>
            <ul>
                <li>Text Classification Overview</li>
                <li>Naive Bayes Classifier</li>
                <li>Logistic Regression</li>
                <li>Support Vector Machines (SVM)</li>
                <li>Feature Representation</li>
                <li>Evaluation Metrics</li>
            </ul>
        </div>
    </div>
</div>

<!-- Slide 4: Text Classification Overview -->
<div class="slide">
    <h2>üìù Text Classification</h2>
    <div class="content-box">
        <h3>Definisi</h3>
        <p><strong>Text Classification</strong> adalah tugas mengassign predefined categories/labels ke dokumen teks.</p>
    </div>

    <div class="example-box">
        <h4>Contoh Aplikasi:</h4>
        <div class="grid-2">
            <div>
                <h4>Sentiment Analysis:</h4>
                <p>"This movie is amazing!" ‚Üí <span class="highlight">Positive</span></p>
                <p>"Worst product ever." ‚Üí <span class="highlight">Negative</span></p>
            </div>
            <div>
                <h4>Spam Detection:</h4>
                <p>"Win $1000 now!!!" ‚Üí <span class="highlight">Spam</span></p>
                <p>"Meeting at 3pm" ‚Üí <span class="highlight">Not Spam</span></p>
            </div>
            <div>
                <h4>Topic Classification:</h4>
                <p>"Python 3.9 released" ‚Üí <span class="highlight">Technology</span></p>
                <p>"Lakers win championship" ‚Üí <span class="highlight">Sports</span></p>
            </div>
            <div>
                <h4>Language Detection:</h4>
                <p>"Bonjour!" ‚Üí <span class="highlight">French</span></p>
                <p>"Hola!" ‚Üí <span class="highlight">Spanish</span></p>
            </div>
        </div>
    </div>

    <div class="content-box">
        <h3>Supervised Learning Pipeline:</h3>
        <ol>
            <li><strong>Training Data:</strong> Dokumen dengan labels yang sudah diketahui</li>
            <li><strong>Feature Extraction:</strong> Convert text ke numerical representation</li>
            <li><strong>Model Training:</strong> Learn patterns dari training data</li>
            <li><strong>Prediction:</strong> Classify dokumen baru</li>
            <li><strong>Evaluation:</strong> Measure performance dengan test data</li>
        </ol>
    </div>
</div>

<!-- Slide 5: Feature Representation -->
<div class="slide">
    <h2>üî¢ Feature Representation untuk Text</h2>
    <div class="content-box">
        <h3>Challenge:</h3>
        <p>Machine learning algorithms bekerja dengan <strong>numbers</strong>, bukan text. Kita perlu convert text menjadi <strong>feature vectors</strong>.</p>
    </div>

    <div class="example-box">
        <h4>1. Bag of Words (BoW)</h4>
        <p>Represent dokumen sebagai "bag" (multiset) dari kata-katanya, <strong>ignore grammar dan word order</strong>.</p>

        <p><strong>Corpus:</strong></p>
        <ul>
            <li>D1: "I love NLP"</li>
            <li>D2: "I love machine learning"</li>
            <li>D3: "NLP is amazing"</li>
        </ul>

        <p><strong>Vocabulary:</strong> {I, love, NLP, machine, learning, is, amazing}</p>

        <table>
            <tr>
                <th>Document</th>
                <th>I</th>
                <th>love</th>
                <th>NLP</th>
                <th>machine</th>
                <th>learning</th>
                <th>is</th>
                <th>amazing</th>
            </tr>
            <tr>
                <td>D1</td>
                <td>1</td>
                <td>1</td>
                <td>1</td>
                <td>0</td>
                <td>0</td>
                <td>0</td>
                <td>0</td>
            </tr>
            <tr>
                <td>D2</td>
                <td>1</td>
                <td>1</td>
                <td>0</td>
                <td>1</td>
                <td>1</td>
                <td>0</td>
                <td>0</td>
            </tr>
            <tr>
                <td>D3</td>
                <td>0</td>
                <td>0</td>
                <td>1</td>
                <td>0</td>
                <td>0</td>
                <td>1</td>
                <td>1</td>
            </tr>
        </table>
    </div>

    <div class="example-box">
        <h4>2. TF-IDF (Term Frequency - Inverse Document Frequency)</h4>
        <p>Weight kata berdasarkan importance: frequent dalam dokumen tapi rare di corpus secara keseluruhan.</p>

        <div class="formula-box">
            <p style="font-size: 1.3em;">
                \[TF\text{-}IDF(t, d) = TF(t, d) \times IDF(t)\]
            </p>
            <p style="font-size: 1.2em;">
                \[TF(t, d) = \frac{\text{count of } t \text{ in } d}{\text{total words in } d}\]
            </p>
            <p style="font-size: 1.2em;">
                \[IDF(t) = \log\frac{\text{total documents}}{\text{documents containing } t}\]
            </p>
        </div>

        <p><strong>Interpretasi:</strong> Kata yang sering muncul di satu dokumen tapi jarang di dokumen lain = <span class="highlight">more informative</span></p>
    </div>
</div>

<!-- Slide 6: Naive Bayes Classifier -->
<div class="slide">
    <h2>üé≤ Naive Bayes Classifier</h2>
    <div class="content-box">
        <h3>Konsep Dasar</h3>
        <p>Classifier probabilistik berdasarkan <strong>Bayes' Theorem</strong> dengan <strong>"naive" conditional independence assumption</strong>.</p>
    </div>

    <div class="formula-box">
        <h4>Bayes' Theorem:</h4>
        <p style="font-size: 1.5em; margin: 20px 0;">
            \[P(c|d) = \frac{P(d|c) \times P(c)}{P(d)}\]
        </p>
        <p>Di mana:</p>
        <ul style="text-align: left; margin-left: 40px;">
            <li><strong>P(c|d)</strong> = Probability class c given document d (posterior)</li>
            <li><strong>P(d|c)</strong> = Probability document d given class c (likelihood)</li>
            <li><strong>P(c)</strong> = Prior probability dari class c</li>
            <li><strong>P(d)</strong> = Probability dari document (evidence)</li>
        </ul>
    </div>

    <div class="important-box">
        <h4>"Naive" Assumption:</h4>
        <p>Semua features (kata) adalah <strong>conditionally independent</strong> given the class.</p>
        <p style="font-size: 1.3em; margin-top: 15px;">
            \[P(d|c) = P(w_1, w_2, ..., w_n | c) = \prod_{i=1}^{n} P(w_i | c)\]
        </p>
        <p>Assumption ini <strong>"naive"</strong> karena kata-kata dalam dokumen sebenarnya <strong>tidak independent</strong>, tapi simplifikasi ini bekerja surprisingly well dalam praktik!</p>
    </div>
</div>

<!-- Slide 7: Naive Bayes Classification -->
<div class="slide">
    <h2>üßÆ Naive Bayes: Klasifikasi</h2>
    <div class="formula-box">
        <h4>Decision Rule:</h4>
        <p>Pilih class dengan posterior probability tertinggi:</p>
        <p style="font-size: 1.4em; margin: 20px 0;">
            \[c^* = \arg\max_c P(c|d) = \arg\max_c P(c) \prod_{i=1}^{n} P(w_i|c)\]
        </p>
        <p><small>(kita ignore P(d) karena sama untuk semua classes)</small></p>
    </div>

    <div class="example-box">
        <h4>üìù Contoh: Spam Classification</h4>
        <p><strong>Training Data:</strong></p>
        <table>
            <tr>
                <th>Email</th>
                <th>Class</th>
            </tr>
            <tr><td>"free money now"</td><td>Spam</td></tr>
            <tr><td>"click here win prize"</td><td>Spam</td></tr>
            <tr><td>"meeting at 2pm"</td><td>Not Spam</td></tr>
            <tr><td>"project deadline tomorrow"</td><td>Not Spam</td></tr>
        </table>

        <p><strong>Prior Probabilities:</strong></p>
        <p>P(Spam) = 2/4 = 0.5</p>
        <p>P(Not Spam) = 2/4 = 0.5</p>

        <p><strong>Word Probabilities (dengan Laplace smoothing):</strong></p>
        <table>
            <tr>
                <th>Word</th>
                <th>P(word|Spam)</th>
                <th>P(word|Not Spam)</th>
            </tr>
            <tr><td>free</td><td>0.15</td><td>0.02</td></tr>
            <tr><td>money</td><td>0.15</td><td>0.02</td></tr>
            <tr><td>meeting</td><td>0.02</td><td>0.15</td></tr>
            <tr><td>project</td><td>0.02</td><td>0.15</td></tr>
        </table>

        <p><strong>Test:</strong> "free project"</p>
        <p>P(Spam | "free project") ‚àù P(Spam) √ó P(free|Spam) √ó P(project|Spam)</p>
        <p>= 0.5 √ó 0.15 √ó 0.02 = <strong>0.0015</strong></p>

        <p>P(Not Spam | "free project") ‚àù P(Not Spam) √ó P(free|Not Spam) √ó P(project|Not Spam)</p>
        <p>= 0.5 √ó 0.02 √ó 0.15 = <strong>0.0015</strong></p>

        <p><strong>Result:</strong> Tie! (dalam praktik, bisa gunakan prior atau default class)</p>
    </div>
</div>

<!-- Slide 8: Naive Bayes Variants -->
<div class="slide">
    <h2>üîÄ Varian Naive Bayes</h2>
    <div class="grid-2">
        <div class="content-box">
            <h4>1. Multinomial Naive Bayes</h4>
            <p><strong>Use case:</strong> Document classification dengan word counts</p>
            <p>Features adalah word frequencies (count berapa kali kata muncul)</p>
            <div class="formula-box">
                <p>\[P(w_i|c) = \frac{\text{count}(w_i, c) + \alpha}{\sum_w \text{count}(w, c) + \alpha |V|}\]</p>
                <p><small>Œ± = smoothing parameter (biasanya 1 untuk Laplace)</small></p>
            </div>
            <p>‚úÖ <strong>Best for:</strong> Text classification, spam detection</p>
        </div>

        <div class="content-box">
            <h4>2. Bernoulli Naive Bayes</h4>
            <p><strong>Use case:</strong> Binary feature vectors</p>
            <p>Features adalah presence/absence (1/0) dari kata</p>
            <div class="formula-box">
                <p>\[P(w_i|c) = P(w_i \text{ present in } c)\]</p>
            </div>
            <p>‚úÖ <strong>Best for:</strong> Short texts, binary features</p>
        </div>

        <div class="content-box">
            <h4>3. Gaussian Naive Bayes</h4>
            <p><strong>Use case:</strong> Continuous features</p>
            <p>Assumes features follow Gaussian (normal) distribution</p>
            <div class="formula-box">
                <p>\[P(x_i|c) = \frac{1}{\sqrt{2\pi\sigma_c^2}} \exp\left(-\frac{(x_i - \mu_c)^2}{2\sigma_c^2}\right)\]</p>
            </div>
            <p>‚úÖ <strong>Best for:</strong> Real-valued features (less common in NLP)</p>
        </div>
    </div>

    <div class="important-box">
        <h4>‚öñÔ∏è Pros & Cons</h4>
        <p><strong>Advantages:</strong></p>
        <ul>
            <li>‚úÖ Simple dan fast (training & prediction)</li>
            <li>‚úÖ Works well dengan high-dimensional data</li>
            <li>‚úÖ Requires small training data</li>
            <li>‚úÖ Interpretable (probabilistic)</li>
        </ul>
        <p><strong>Disadvantages:</strong></p>
        <ul>
            <li>‚ùå Naive independence assumption sering violated</li>
            <li>‚ùå Tidak bisa learn feature interactions</li>
            <li>‚ùå Zero-frequency problem (solved dengan smoothing)</li>
        </ul>
    </div>
</div>

<!-- Slide 9: Logistic Regression -->
<div class="slide">
    <h2>üìà Logistic Regression</h2>
    <div class="content-box">
        <h3>Konsep</h3>
        <p><strong>Logistic Regression</strong> adalah discriminative classifier yang memprediksi probability bahwa input belongs to a particular class.</p>
        <p><strong>Perbedaan dengan Naive Bayes:</strong></p>
        <ul>
            <li><strong>Naive Bayes:</strong> Generative model - models P(x|y) dan P(y)</li>
            <li><strong>Logistic Regression:</strong> Discriminative model - directly models P(y|x)</li>
        </ul>
    </div>

    <div class="formula-box">
        <h4>Sigmoid Function:</h4>
        <p style="font-size: 1.4em; margin: 20px 0;">
            \[\sigma(z) = \frac{1}{1 + e^{-z}}\]
        </p>
        <p>Maps any real value to (0, 1) range - perfect untuk probabilities!</p>
    </div>

    <div class="formula-box">
        <h4>Logistic Regression Model:</h4>
        <p style="font-size: 1.3em; margin: 20px 0;">
            \[P(y=1|x) = \sigma(w^T x + b) = \frac{1}{1 + e^{-(w^T x + b)}}\]
        </p>
        <p>Di mana:</p>
        <ul style="text-align: left; margin-left: 40px;">
            <li><strong>x</strong> = feature vector (TF-IDF, BoW, etc.)</li>
            <li><strong>w</strong> = weight vector (learned)</li>
            <li><strong>b</strong> = bias term (learned)</li>
        </ul>
    </div>

    <div id="sigmoidPlot" class="plot-container"></div>
</div>

<!-- Slide 10: Logistic Regression Training -->
<div class="slide">
    <h2>üéì Training Logistic Regression</h2>
    <div class="content-box">
        <h3>Goal:</h3>
        <p>Find weights <strong>w</strong> dan bias <strong>b</strong> yang maximize likelihood dari training data.</p>
    </div>

    <div class="formula-box">
        <h4>Loss Function: Cross-Entropy Loss</h4>
        <p style="font-size: 1.3em; margin: 20px 0;">
            \[L(w, b) = -\frac{1}{N}\sum_{i=1}^{N} \left[y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)\right]\]
        </p>
        <p>Di mana:</p>
        <ul style="text-align: left; margin-left: 40px;">
            <li>\(y_i\) = true label (0 or 1)</li>
            <li>\(\hat{y}_i = P(y=1|x_i)\) = predicted probability</li>
        </ul>
    </div>

    <div class="content-box">
        <h3>Optimization: Gradient Descent</h3>
        <p>Update weights iteratively untuk minimize loss:</p>
        <div class="formula-box">
            <p style="font-size: 1.3em;">
                \[w := w - \eta \nabla_w L\]
            </p>
            <p style="font-size: 1.3em;">
                \[b := b - \eta \nabla_b L\]
            </p>
            <p>Œ∑ = learning rate</p>
        </div>

        <p><strong>Gradient untuk Logistic Regression:</strong></p>
        <div class="formula-box">
            <p style="font-size: 1.2em;">
                \[\nabla_w L = \frac{1}{N}\sum_{i=1}^{N} (\hat{y}_i - y_i) x_i\]
            </p>
        </div>
    </div>

    <div class="success-box">
        <h4>‚úÖ Advantages:</h4>
        <ul>
            <li>Probabilistic interpretation</li>
            <li>Efficient untuk large datasets</li>
            <li>Dapat handle feature correlations (tidak seperti Naive Bayes)</li>
            <li>Regularization (L1, L2) untuk prevent overfitting</li>
        </ul>
    </div>
</div>

<!-- Slide 11: Regularization -->
<div class="slide">
    <h2>üõ°Ô∏è Regularization</h2>
    <div class="content-box">
        <h3>Problem: Overfitting</h3>
        <p>Model terlalu complex, fit training data perfectly tapi gagal di test data.</p>
        <p><strong>Solution:</strong> Add penalty untuk large weights</p>
    </div>

    <div class="grid-2">
        <div class="content-box">
            <h4>L2 Regularization (Ridge)</h4>
            <div class="formula-box">
                <p>\[L_{L2}(w) = L(w) + \lambda \sum_{i} w_i^2\]</p>
            </div>
            <p><strong>Effect:</strong> Shrinks all weights, prefers small weights</p>
            <p>‚úÖ Good for preventing overfitting</p>
            <p>‚úÖ All features retained</p>
        </div>

        <div class="content-box">
            <h4>L1 Regularization (Lasso)</h4>
            <div class="formula-box">
                <p>\[L_{L1}(w) = L(w) + \lambda \sum_{i} |w_i|\]</p>
            </div>
            <p><strong>Effect:</strong> Drives some weights to exactly 0</p>
            <p>‚úÖ Feature selection (sparse model)</p>
            <p>‚úÖ Good untuk interpretability</p>
        </div>
    </div>

    <div class="important-box">
        <h4>Hyperparameter Œª (regularization strength):</h4>
        <ul>
            <li><strong>Œª = 0:</strong> No regularization (risk overfitting)</li>
            <li><strong>Œª small:</strong> Weak regularization</li>
            <li><strong>Œª large:</strong> Strong regularization (risk underfitting)</li>
        </ul>
        <p>Pilih Œª optimal via <strong>cross-validation</strong></p>
    </div>
</div>

<!-- Slide 12: Support Vector Machines -->
<div class="slide">
    <h2>‚öîÔ∏è Support Vector Machines (SVM)</h2>
    <div class="content-box">
        <h3>Ide Utama</h3>
        <p><strong>SVM</strong> finds the <span class="highlight">optimal hyperplane</span> yang memisahkan classes dengan <strong>maximum margin</strong>.</p>
    </div>

    <div class="grid-2">
        <div class="content-box">
            <h4>Linear SVM</h4>
            <p>Cari decision boundary (hyperplane) yang maximize distance ke nearest data points dari kedua classes.</p>
            <p><strong>Support Vectors:</strong> Data points yang paling dekat dengan hyperplane</p>
            <div class="formula-box">
                <p>Hyperplane: \(w^T x + b = 0\)</p>
                <p>Margin: \(\frac{2}{||w||}\)</p>
            </div>
            <p>Goal: <strong>Maximize margin</strong> = minimize ||w||</p>
        </div>

        <div class="content-box">
            <h4>Non-linear SVM (Kernel Trick)</h4>
            <p>Untuk data yang tidak linearly separable, map ke higher-dimensional space.</p>
            <p><strong>Popular Kernels:</strong></p>
            <ul>
                <li><strong>Linear:</strong> K(x, x') = x^T x'</li>
                <li><strong>Polynomial:</strong> K(x, x') = (x^T x' + c)^d</li>
                <li><strong>RBF (Gaussian):</strong> K(x, x') = exp(-Œ≥||x - x'||¬≤)</li>
            </ul>
        </div>
    </div>

    <div class="formula-box">
        <h4>SVM Optimization Problem:</h4>
        <p style="font-size: 1.3em;">
            \[\min_{w,b} \frac{1}{2}||w||^2\]
        </p>
        <p>Subject to: \(y_i(w^T x_i + b) \geq 1\) for all i</p>
        <p><strong>Soft Margin SVM</strong> (allows some misclassification):</p>
        <p style="font-size: 1.2em;">
            \[\min_{w,b,\xi} \frac{1}{2}||w||^2 + C\sum_{i}\xi_i\]
        </p>
        <p>C = trade-off between margin size dan misclassification</p>
    </div>
</div>

<!-- Slide 13: SVM for Text Classification -->
<div class="slide">
    <h2>üìÑ SVM untuk Text Classification</h2>
    <div class="success-box">
        <h3>Mengapa SVM Bagus untuk Text?</h3>
        <ul>
            <li>‚úÖ <strong>High-dimensional spaces:</strong> Text features (BoW, TF-IDF) = thousands of dimensions</li>
            <li>‚úÖ <strong>Sparse data:</strong> Most documents hanya mengandung small fraction dari vocabulary</li>
            <li>‚úÖ <strong>Linearly separable:</strong> Many text classification problems are (approximately) linearly separable di high dimensions</li>
            <li>‚úÖ <strong>Regularization:</strong> Maximum margin principle adalah form of regularization</li>
        </ul>
    </div>

    <div class="example-box">
        <h4>üìù Example: Sentiment Classification dengan SVM</h4>
        <p><strong>Step 1:</strong> Feature Extraction (TF-IDF)</p>
        <div class="code-block">
from sklearn.feature_extraction.text import TfidfVectorizer

reviews = [
    "This movie is amazing and wonderful",
    "Terrible film, waste of time",
    "Brilliant acting and great story",
    "Boring and disappointing"
]
labels = [1, 0, 1, 0]  # 1=positive, 0=negative

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(reviews)
        </div>

        <p><strong>Step 2:</strong> Train SVM</p>
        <div class="code-block">
from sklearn.svm import SVC

# Linear kernel SVM
svm = SVC(kernel='linear', C=1.0)
svm.fit(X, labels)
        </div>

        <p><strong>Step 3:</strong> Predict</p>
        <div class="code-block">
new_review = ["Great movie, loved it"]
X_new = vectorizer.transform(new_review)
prediction = svm.predict(X_new)
print(f"Sentiment: {'Positive' if prediction[0] == 1 else 'Negative'}")
        </div>
    </div>

    <div class="content-box">
        <h4>‚öôÔ∏è Hyperparameters untuk Tuning:</h4>
        <ul>
            <li><strong>C:</strong> Regularization strength (larger C = less regularization)</li>
            <li><strong>kernel:</strong> linear, poly, rbf, sigmoid</li>
            <li><strong>gamma:</strong> For RBF kernel (larger Œ≥ = more complex decision boundary)</li>
        </ul>
    </div>
</div>

<!-- Slide 14: Comparison of Algorithms -->
<div class="slide">
    <h2>‚öñÔ∏è Perbandingan Algoritma</h2>
    <table>
        <thead>
            <tr>
                <th>Aspect</th>
                <th>Naive Bayes</th>
                <th>Logistic Regression</th>
                <th>SVM</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><strong>Type</strong></td>
                <td>Generative</td>
                <td>Discriminative</td>
                <td>Discriminative</td>
            </tr>
            <tr>
                <td><strong>Training Speed</strong></td>
                <td>‚ö° Very Fast</td>
                <td>‚ö° Fast</td>
                <td>üê¢ Slower (esp. non-linear)</td>
            </tr>
            <tr>
                <td><strong>Prediction Speed</strong></td>
                <td>‚ö° Very Fast</td>
                <td>‚ö° Very Fast</td>
                <td>‚ö° Fast (linear), üê¢ Slow (kernel)</td>
            </tr>
            <tr>
                <td><strong>Data Requirements</strong></td>
                <td>‚úÖ Works with small data</td>
                <td>Moderate</td>
                <td>Moderate to Large</td>
            </tr>
            <tr>
                <td><strong>Feature Dependencies</strong></td>
                <td>‚ùå Assumes independence</td>
                <td>‚úÖ Handles correlated features</td>
                <td>‚úÖ Handles correlated features</td>
            </tr>
            <tr>
                <td><strong>Interpretability</strong></td>
                <td>‚úÖ High (probabilities)</td>
                <td>‚úÖ High (weights)</td>
                <td>‚ö†Ô∏è Medium (support vectors)</td>
            </tr>
            <tr>
                <td><strong>Regularization</strong></td>
                <td>Smoothing only</td>
                <td>‚úÖ L1, L2</td>
                <td>‚úÖ Built-in (margin)</td>
            </tr>
            <tr>
                <td><strong>Non-linear Boundaries</strong></td>
                <td>‚ùå Linear only</td>
                <td>‚ùå Linear (need feature engineering)</td>
                <td>‚úÖ Yes (kernel trick)</td>
            </tr>
            <tr>
                <td><strong>Multiclass</strong></td>
                <td>‚úÖ Native</td>
                <td>‚úÖ Native (softmax)</td>
                <td>‚ö†Ô∏è One-vs-rest or one-vs-one</td>
            </tr>
            <tr>
                <td><strong>Best For</strong></td>
                <td>Baseline, small data, fast prototype</td>
                <td>General text classification</td>
                <td>High-dim, complex boundaries</td>
            </tr>
        </tbody>
    </table>

    <div class="important-box">
        <h4>üí° Rekomendasi Praktis:</h4>
        <ul>
            <li><strong>Start with Naive Bayes:</strong> Fast baseline, surprisingly effective</li>
            <li><strong>Try Logistic Regression:</strong> Often best performance/simplicity trade-off</li>
            <li><strong>Use SVM:</strong> Untuk complex problems, ketika LR tidak cukup</li>
            <li><strong>Always:</strong> Use cross-validation dan hyperparameter tuning!</li>
        </ul>
    </div>
</div>

<!-- Slide 15: Evaluation Metrics -->
<div class="slide">
    <h2>üìä Evaluasi Model Classification</h2>
    <div class="content-box">
        <h3>Confusion Matrix</h3>
        <table>
            <tr>
                <th></th>
                <th>Predicted Positive</th>
                <th>Predicted Negative</th>
            </tr>
            <tr>
                <th>Actual Positive</th>
                <td style="background: #c8e6c9;"><strong>True Positive (TP)</strong></td>
                <td style="background: #ffcdd2;"><strong>False Negative (FN)</strong></td>
            </tr>
            <tr>
                <th>Actual Negative</th>
                <td style="background: #ffcdd2;"><strong>False Positive (FP)</strong></td>
                <td style="background: #c8e6c9;"><strong>True Negative (TN)</strong></td>
            </tr>
        </table>
    </div>

    <div class="grid-2">
        <div class="formula-box">
            <h4>Accuracy</h4>
            <p>\[\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}\]</p>
            <p>Overall correctness</p>
            <p>‚ö†Ô∏è Misleading untuk imbalanced data!</p>
        </div>

        <div class="formula-box">
            <h4>Precision</h4>
            <p>\[\text{Precision} = \frac{TP}{TP + FP}\]</p>
            <p>Of all predicted positive, berapa yang benar?</p>
            <p>‚úÖ Important ketika FP costly</p>
        </div>

        <div class="formula-box">
            <h4>Recall (Sensitivity)</h4>
            <p>\[\text{Recall} = \frac{TP}{TP + FN}\]</p>
            <p>Of all actual positive, berapa yang terdeteksi?</p>
            <p>‚úÖ Important ketika FN costly</p>
        </div>

        <div class="formula-box">
            <h4>F1-Score</h4>
            <p>\[F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}\]</p>
            <p>Harmonic mean dari Precision & Recall</p>
            <p>‚úÖ Balanced metric</p>
        </div>
    </div>

    <div class="example-box">
        <h4>Contoh:</h4>
        <p>Spam detection: 90 spam correctly identified (TP), 10 spam missed (FN), 5 ham marked as spam (FP), 895 ham correctly identified (TN)</p>
        <ul>
            <li>Accuracy = (90+895)/(90+895+5+10) = <strong>98.5%</strong></li>
            <li>Precision = 90/(90+5) = <strong>94.7%</strong></li>
            <li>Recall = 90/(90+10) = <strong>90%</strong></li>
            <li>F1 = 2 √ó (0.947 √ó 0.9)/(0.947 + 0.9) = <strong>92.3%</strong></li>
        </ul>
    </div>
</div>

<!-- Slide 16: Cross-Validation -->
<div class="slide">
    <h2>üîÑ Cross-Validation</h2>
    <div class="content-box">
        <h3>Why Cross-Validation?</h3>
        <p>Single train/test split bisa give misleading results due to randomness.</p>
        <p><strong>Solution:</strong> Test model pada multiple different splits</p>
    </div>

    <div class="content-box">
        <h4>K-Fold Cross-Validation</h4>
        <ol>
            <li>Split data menjadi K equal-sized folds</li>
            <li>For each fold k = 1 to K:
                <ul>
                    <li>Use fold k sebagai test set</li>
                    <li>Use remaining K-1 folds sebagai training set</li>
                    <li>Train model dan evaluate</li>
                </ul>
            </li>
            <li>Average performance across all K folds</li>
        </ol>
    </div>

    <div class="example-box">
        <h4>üìù Example: 5-Fold CV</h4>
        <div class="code-block">
from sklearn.model_selection import cross_val_score
from sklearn.naive_bayes import MultinomialNB

# Assume X, y are features and labels
nb = MultinomialNB()

# 5-fold cross-validation
scores = cross_val_score(nb, X, y, cv=5, scoring='f1')

print(f"F1 scores: {scores}")
print(f"Mean F1: {scores.mean():.3f} (+/- {scores.std():.3f})")
        </div>
        <p><strong>Output example:</strong></p>
        <p>F1 scores: [0.85, 0.87, 0.83, 0.86, 0.84]</p>
        <p>Mean F1: <strong>0.850 (+/- 0.014)</strong></p>
    </div>

    <div class="important-box">
        <h4>Best Practices:</h4>
        <ul>
            <li>Use <strong>stratified k-fold</strong> untuk imbalanced datasets (maintains class distribution)</li>
            <li>Common K values: 5 atau 10</li>
            <li>Use CV untuk hyperparameter tuning (GridSearchCV, RandomizedSearchCV)</li>
            <li>Always keep separate <strong>holdout test set</strong> untuk final evaluation</li>
        </ul>
    </div>
</div>

<!-- Slide 17: Practical Implementation -->
<div class="slide">
    <h2>üíª Complete Pipeline Implementation</h2>
    <div class="code-block">
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

# 1. Load data
data = pd.read_csv('movie_reviews.csv')
X = data['review']
y = data['sentiment']  # 0=negative, 1=positive

# 2. Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 3. Feature extraction
vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

# 4. Train multiple models
models = {
    'Naive Bayes': MultinomialNB(),
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'SVM': LinearSVC()
}

results = {}
for name, model in models.items():
    # Train
    model.fit(X_train_vec, y_train)

    # Predict
    y_pred = model.predict(X_test_vec)

    # Evaluate
    print(f"\n{name}:")
    print(classification_report(y_test, y_pred))
    results[name] = model

# 5. Hyperparameter tuning (example for Logistic Regression)
param_grid = {
    'C': [0.1, 1, 10, 100],
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear']
}

grid_search = GridSearchCV(
    LogisticRegression(max_iter=1000),
    param_grid,
    cv=5,
    scoring='f1',
    n_jobs=-1
)

grid_search.fit(X_train_vec, y_train)
print(f"\nBest parameters: {grid_search.best_params_}")
print(f"Best F1 score: {grid_search.best_score_:.3f}")

# 6. Final evaluation with best model
best_model = grid_search.best_estimator_
y_pred_final = best_model.predict(X_test_vec)
print("\nFinal Test Set Performance:")
print(classification_report(y_test, y_pred_final))
    </div>
</div>

<!-- Slide 18: Feature Engineering Tips -->
<div class="slide">
    <h2>üîß Feature Engineering Tips untuk Text</h2>
    <div class="grid-2">
        <div class="content-box">
            <h4>1. Preprocessing</h4>
            <ul>
                <li><strong>Lowercasing:</strong> "Apple" vs "apple"</li>
                <li><strong>Remove punctuation:</strong> "hello!" ‚Üí "hello"</li>
                <li><strong>Remove stop words:</strong> "the", "a", "is"</li>
                <li><strong>Stemming/Lemmatization:</strong> "running" ‚Üí "run"</li>
            </ul>
        </div>

        <div class="content-box">
            <h4>2. N-grams</h4>
            <ul>
                <li><strong>Unigrams:</strong> "not", "good"</li>
                <li><strong>Bigrams:</strong> "not good" (captures negation!)</li>
                <li><strong>Trigrams:</strong> "not very good"</li>
            </ul>
            <p>Trade-off: More n-grams = more features = more data needed</p>
        </div>

        <div class="content-box">
            <h4>3. TF-IDF Parameters</h4>
            <ul>
                <li><strong>max_features:</strong> Limit vocabulary size</li>
                <li><strong>min_df:</strong> Ignore rare terms</li>
                <li><strong>max_df:</strong> Ignore too common terms</li>
                <li><strong>ngram_range:</strong> (1,2) = unigrams + bigrams</li>
            </ul>
        </div>

        <div class="content-box">
            <h4>4. Domain-Specific Features</h4>
            <ul>
                <li><strong>Sentiment:</strong> Count positive/negative words</li>
                <li><strong>Spam:</strong> Number of CAPS, exclamations</li>
                <li><strong>Authorship:</strong> Avg word length, punctuation ratio</li>
            </ul>
        </div>
    </div>

    <div class="example-box">
        <h4>Example with Preprocessing:</h4>
        <div class="code-block">
from sklearn.feature_extraction.text import TfidfVectorizer
import re
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

def preprocess(text):
    # Lowercase
    text = text.lower()
    # Remove special chars
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # Tokenize
    tokens = text.split()
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [w for w in tokens if w not in stop_words]
    # Stemming
    stemmer = PorterStemmer()
    tokens = [stemmer.stem(w) for w in tokens]
    return ' '.join(tokens)

# Apply preprocessing
X_train_clean = X_train.apply(preprocess)

# TF-IDF with n-grams
vectorizer = TfidfVectorizer(
    max_features=10000,
    ngram_range=(1, 2),
    min_df=2,
    max_df=0.8
)
X_train_vec = vectorizer.fit_transform(X_train_clean)
        </div>
    </div>
</div>

<!-- Slide 19: Common Pitfalls -->
<div class="slide">
    <h2>‚ö†Ô∏è Common Pitfalls & Solutions</h2>
    <div class="important-box">
        <h4>1. Data Leakage</h4>
        <p><strong>Problem:</strong> Fitting vectorizer pada entire dataset (train + test)</p>
        <div class="code-block">
# ‚ùå WRONG
X_vec = vectorizer.fit_transform(X)  # Entire dataset
X_train, X_test = train_test_split(X_vec, ...)
        </div>
        <div class="code-block">
# ‚úÖ CORRECT
X_train, X_test = train_test_split(X, ...)
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)  # Only transform!
        </div>
    </div>

    <div class="important-box">
        <h4>2. Imbalanced Classes</h4>
        <p><strong>Problem:</strong> 95% negative, 5% positive ‚Üí model predicts all negative, gets 95% accuracy!</p>
        <p><strong>Solutions:</strong></p>
        <ul>
            <li>Use <strong>stratified sampling</strong></li>
            <li>Use appropriate metrics: <strong>F1, precision, recall</strong> instead of accuracy</li>
            <li><strong>Class weighting:</strong> class_weight='balanced' in sklearn</li>
            <li><strong>Resampling:</strong> oversample minority or undersample majority</li>
        </ul>
    </div>

    <div class="important-box">
        <h4>3. Overfitting</h4>
        <p><strong>Symptoms:</strong> High train accuracy, low test accuracy</p>
        <p><strong>Solutions:</strong></p>
        <ul>
            <li>Use <strong>regularization</strong> (L1/L2)</li>
            <li>Reduce feature dimensionality (max_features)</li>
            <li>Get more training data</li>
            <li>Use simpler model (Naive Bayes instead of SVM)</li>
        </ul>
    </div>

    <div class="important-box">
        <h4>4. Not Tuning Hyperparameters</h4>
        <p><strong>Problem:</strong> Using default parameters</p>
        <p><strong>Solution:</strong> Always use GridSearchCV atau RandomizedSearchCV</p>
    </div>
</div>

<!-- Slide 20: Real-World Case Study -->
<div class="slide">
    <h2>üåç Case Study: Sentiment Analysis pada Amazon Reviews</h2>
    <div class="content-box">
        <h3>Problem</h3>
        <p>Classify Amazon product reviews sebagai positive atau negative untuk help customers quickly assess product quality.</p>
        <p><strong>Dataset:</strong> 10,000 reviews (balanced: 5000 positive, 5000 negative)</p>
    </div>

    <div class="example-box">
        <h4>Approach & Results:</h4>
        <table>
            <tr>
                <th>Model</th>
                <th>Features</th>
                <th>Accuracy</th>
                <th>F1-Score</th>
                <th>Training Time</th>
            </tr>
            <tr>
                <td>Naive Bayes</td>
                <td>BoW (5000 features)</td>
                <td>83.2%</td>
                <td>0.831</td>
                <td>0.5s</td>
            </tr>
            <tr>
                <td>Naive Bayes</td>
                <td>TF-IDF unigrams</td>
                <td>85.1%</td>
                <td>0.849</td>
                <td>0.6s</td>
            </tr>
            <tr>
                <td>Naive Bayes</td>
                <td>TF-IDF + bigrams</td>
                <td>87.3%</td>
                <td>0.871</td>
                <td>1.2s</td>
            </tr>
            <tr style="background: #e8f5e9;">
                <td><strong>Logistic Regression</strong></td>
                <td><strong>TF-IDF + bigrams + L2</strong></td>
                <td><strong>89.7%</strong></td>
                <td><strong>0.896</strong></td>
                <td><strong>3.4s</strong></td>
            </tr>
            <tr>
                <td>Linear SVM</td>
                <td>TF-IDF + bigrams</td>
                <td>90.1%</td>
                <td>0.900</td>
                <td>8.2s</td>
            </tr>
            <tr>
                <td>SVM (RBF kernel)</td>
                <td>TF-IDF + bigrams</td>
                <td>88.5%</td>
                <td>0.883</td>
                <td>125s</td>
            </tr>
        </table>
    </div>

    <div class="success-box">
        <h4>‚úÖ Key Findings:</h4>
        <ul>
            <li><strong>Best model:</strong> Linear SVM (90.1% accuracy)</li>
            <li><strong>Best trade-off:</strong> Logistic Regression (89.7% accuracy, 3.4s training)</li>
            <li><strong>Bigrams helped significantly:</strong> +2-4% improvement over unigrams</li>
            <li><strong>RBF kernel didn't help:</strong> Linear was better (text often linearly separable)</li>
            <li><strong>Feature engineering mattered:</strong> TF-IDF > BoW</li>
        </ul>
    </div>

    <div class="content-box">
        <h4>üöÄ Deployment Decision:</h4>
        <p>Chose <strong>Logistic Regression</strong> untuk production karena:</p>
        <ul>
            <li>Near-best performance (only 0.4% lower than SVM)</li>
            <li>2.4√ó faster training (easier to retrain)</li>
            <li>More interpretable (can examine feature weights)</li>
            <li>Gives probability scores (not just binary prediction)</li>
        </ul>
    </div>
</div>

<!-- Slide 21: Assignment -->
<div class="slide">
    <h2>üìù Tugas Praktikum</h2>
    <div class="content-box">
        <h3>Tugas Terbimbing (2 jam)</h3>
        <p><strong>Implementasi Text Classification dengan Naive Bayes dan Logistic Regression</strong></p>
        <h4>Langkah-langkah:</h4>
        <ol>
            <li>Download dataset: <a href="https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews">IMDB Movie Reviews</a> atau gunakan dataset sendiri</li>
            <li>Preprocessing: cleaning, tokenization</li>
            <li>Feature extraction: BoW dan TF-IDF</li>
            <li>Train Naive Bayes classifier</li>
            <li>Train Logistic Regression classifier</li>
            <li>Compare performance (accuracy, precision, recall, F1)</li>
            <li>Visualize confusion matrix</li>
            <li>Analyze which words are most indicative of each class</li>
        </ol>
    </div>

    <div class="content-box">
        <h3>Tugas Mandiri (4 jam)</h3>
        <p><strong>Sentiment Analysis Project dengan SVM</strong></p>
        <h4>Tasks:</h4>
        <ol>
            <li>Collect atau gunakan dataset sentiment (Twitter, product reviews, dll)</li>
            <li>Implement complete ML pipeline:
                <ul>
                    <li>Data preprocessing</li>
                    <li>Feature engineering (TF-IDF dengan n-grams)</li>
                    <li>Train multiple models (NB, LR, SVM)</li>
                    <li>Hyperparameter tuning dengan GridSearchCV</li>
                    <li>5-fold cross-validation</li>
                </ul>
            </li>
            <li>Compare all models dan pilih yang terbaik</li>
            <li>Error analysis: examine misclassified examples</li>
            <li>Write report dengan visualizations</li>
        </ol>
    </div>

    <div class="example-box">
        <h4>üìä Deliverables:</h4>
        <ul>
            <li>Jupyter notebook dengan complete code dan explanations</li>
            <li>Report (PDF, 3-4 pages):
                <ul>
                    <li>Dataset description dan exploratory analysis</li>
                    <li>Methodology (preprocessing, features, models)</li>
                    <li>Results comparison table</li>
                    <li>Confusion matrices dan plots</li>
                    <li>Error analysis</li>
                    <li>Conclusions dan insights</li>
                </ul>
            </li>
        </ul>
    </div>
</div>

<!-- Slide 22: Resources -->
<div class="slide">
    <h2>üìö Referensi dan Sumber Belajar</h2>
    <div class="content-box">
        <h3>Textbooks:</h3>
        <ul>
            <li>Christopher Bishop - <em>"Pattern Recognition and Machine Learning"</em></li>
            <li>Trevor Hastie et al. - <em>"The Elements of Statistical Learning"</em></li>
            <li>Jurafsky & Martin - <em>"Speech and Language Processing"</em> (Ch. 4-5)</li>
        </ul>
    </div>

    <div class="content-box">
        <h3>Online Courses:</h3>
        <ul>
            <li>Andrew Ng - Machine Learning (Coursera)</li>
            <li>Stanford CS229: Machine Learning</li>
            <li>fast.ai - Practical Deep Learning for Coders</li>
        </ul>
    </div>

    <div class="content-box">
        <h3>Libraries & Tools:</h3>
        <ul>
            <li><strong>scikit-learn:</strong> ML library untuk Python (NB, LR, SVM, evaluation)</li>
            <li><strong>NLTK & spaCy:</strong> Text preprocessing</li>
            <li><strong>pandas:</strong> Data manipulation</li>
            <li><strong>matplotlib & seaborn:</strong> Visualization</li>
        </ul>
    </div>

    <div class="content-box">
        <h3>Datasets:</h3>
        <ul>
            <li>IMDB Movie Reviews (sentiment)</li>
            <li>20 Newsgroups (topic classification)</li>
            <li>SMS Spam Collection (spam detection)</li>
            <li>Yelp Reviews (sentiment)</li>
            <li>AG News (news classification)</li>
        </ul>
    </div>
</div>

<!-- Slide 23: Summary -->
<div class="slide">
    <h2>üìå Kesimpulan</h2>
    <div class="content-box">
        <h3>Key Takeaways:</h3>
        <ol style="font-size: 1.3em; line-height: 2;">
            <li>Machine Learning memungkinkan <strong>automated text classification</strong> yang scalable dan akurat</li>
            <li><strong>Feature representation</strong> (BoW, TF-IDF) adalah crucial untuk convert text ke numbers</li>
            <li><strong>Naive Bayes:</strong> Simple, fast, good baseline despite "naive" assumption</li>
            <li><strong>Logistic Regression:</strong> Discriminative model dengan good performance dan interpretability</li>
            <li><strong>SVM:</strong> Powerful untuk high-dimensional text data dengan maximum margin principle</li>
            <li><strong>Evaluation:</strong> Use appropriate metrics (F1, precision, recall) dan cross-validation</li>
            <li><strong>No free lunch:</strong> Different algorithms work better untuk different problems - always experiment!</li>
        </ol>
    </div>

    <div class="important-box">
        <h4>üîú Next Week:</h4>
        <p><strong>Deep Learning untuk NLP</strong></p>
        <p>Neural Networks, Word Embeddings, Feedforward Networks, Backpropagation</p>
    </div>
</div>

<!-- Slide 24: Q&A -->
<div class="slide">
    <h2>‚ùì Pertanyaan?</h2>
    <div style="text-align: center; margin: 60px 0;">
        <h3 style="font-size: 2.5em; margin-bottom: 30px;">Terima Kasih! üôè</h3>
        <p style="font-size: 1.5em; color: #666;">
            Session 10: Machine Learning untuk NLP
        </p>
    </div>
    <div class="footer">
        <p>Natural Language Processing</p>
        <p>Universitas Mercu Buana</p>
        <p>Program Studi Teknik Informatika</p>
    </div>
</div>

<script>
// Plotly visualization for Sigmoid function
const xValues = [];
const yValues = [];
for (let x = -10; x <= 10; x += 0.1) {
    xValues.push(x);
    yValues.push(1 / (1 + Math.exp(-x)));
}

const sigmoidData = [{
    x: xValues,
    y: yValues,
    type: 'scatter',
    mode: 'lines',
    line: {color: '#667eea', width: 3},
    name: 'œÉ(z) = 1/(1+e^(-z))'
}];

const sigmoidLayout = {
    title: {
        text: 'Sigmoid Function',
        font: {size: 20, color: '#333'}
    },
    xaxis: {
        title: 'z (w^T x + b)',
        titlefont: {size: 16},
        zeroline: true
    },
    yaxis: {
        title: 'œÉ(z)',
        titlefont: {size: 16},
        range: [-0.1, 1.1]
    },
    shapes: [
        {
            type: 'line',
            x0: -10, x1: 10,
            y0: 0.5, y1: 0.5,
            line: {dash: 'dash', color: '#999'}
        }
    ],
    annotations: [
        {
            x: 5, y: 0.5,
            text: 'Decision boundary (0.5)',
            showarrow: false,
            font: {size: 12}
        }
    ],
    paper_bgcolor: 'rgba(0,0,0,0)',
    plot_bgcolor: 'rgba(240,240,240,0.5)'
};

Plotly.newPlot('sigmoidPlot', sigmoidData, sigmoidLayout, {responsive: true});

// Smooth scrolling
document.querySelectorAll('a[href^="#"]').forEach(anchor => {
    anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const target = document.querySelector(this.getAttribute('href'));
        if (target) {
            target.scrollIntoView({
                behavior: 'smooth',
                block: 'start'
            });
        }
    });
});
</script>

</body>
</html>
