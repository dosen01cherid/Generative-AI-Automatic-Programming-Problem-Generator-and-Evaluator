{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Ollama Setup on Google Colab with Cloudflare Tunnel\n",
    "\n",
    "This notebook will:\n",
    "1. Install Ollama on Google Colab\n",
    "2. Set up Cloudflare Tunnel for public access\n",
    "3. Download and run large language models\n",
    "\n",
    "**Important Notes:**\n",
    "- This requires a Colab session with sufficient resources\n",
    "- Models are large and will take time to download\n",
    "- The Cloudflare tunnel will remain active as long as the notebook is running\n",
    "- Free Colab has runtime limits; consider Colab Pro for longer sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section1"
   },
   "source": [
    "## 1. System Information and Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_system"
   },
   "outputs": [],
   "source": [
    "# Check system information\n",
    "!echo \"=== System Information ===\"\n",
    "!uname -a\n",
    "!echo \"\\n=== GPU Information ===\"\n",
    "!nvidia-smi 2>/dev/null || echo \"No GPU detected (CPU mode will be slower)\"\n",
    "!echo \"\\n=== Memory Information ===\"\n",
    "!free -h\n",
    "!echo \"\\n=== Disk Space ===\"\n",
    "!df -h /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2"
   },
   "source": [
    "## 2. Install Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_ollama"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üì¶ Installing Ollama...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Download and install Ollama\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "print(\"\\n‚úÖ Ollama installation completed!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section3"
   },
   "source": [
    "## 3. Start Ollama Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "start_ollama"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "\n",
    "print(\"üöÄ Starting Ollama server...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Start Ollama server in background\n",
    "ollama_process = subprocess.Popen(\n",
    "    ['ollama', 'serve'],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# Wait for server to start\n",
    "print(\"Waiting for Ollama server to start...\")\n",
    "max_retries = 30\n",
    "retry_count = 0\n",
    "\n",
    "while retry_count < max_retries:\n",
    "    try:\n",
    "        response = requests.get('http://localhost:11434/api/tags', timeout=1)\n",
    "        if response.status_code == 200:\n",
    "            print(\"‚úÖ Ollama server is running!\")\n",
    "            break\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    time.sleep(1)\n",
    "    retry_count += 1\n",
    "    print(f\"Attempt {retry_count}/{max_retries}...\", end='\\r')\n",
    "\n",
    "if retry_count >= max_retries:\n",
    "    print(\"\\n‚ùå Failed to start Ollama server\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Server started successfully on http://localhost:11434\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section4"
   },
   "source": [
    "## 4. Install Cloudflare Tunnel (cloudflared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_cloudflared"
   },
   "outputs": [],
   "source": [
    "print(\"‚òÅÔ∏è  Installing Cloudflare Tunnel...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Download and install cloudflared\n",
    "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
    "!dpkg -i cloudflared-linux-amd64.deb\n",
    "\n",
    "# Verify installation\n",
    "!cloudflared --version\n",
    "\n",
    "print(\"\\n‚úÖ Cloudflare Tunnel installed successfully!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section5"
   },
   "source": [
    "## 5. Start Cloudflare Tunnel to Expose Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "start_tunnel"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import re\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "print(\"üåê Starting Cloudflare Tunnel...\")\n",
    "print(\"=\" * 80)\n",
    "print(\"This will create a public URL to access your Ollama instance.\")\n",
    "print(\"The URL will be displayed below when ready.\\n\")\n",
    "\n",
    "# Start cloudflared tunnel\n",
    "tunnel_process = subprocess.Popen(\n",
    "    ['cloudflared', 'tunnel', '--url', 'http://localhost:11434'],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    text=True,\n",
    "    bufsize=1\n",
    ")\n",
    "\n",
    "# Extract the public URL\n",
    "public_url = None\n",
    "print(\"Waiting for tunnel to establish...\")\n",
    "\n",
    "for line in iter(tunnel_process.stdout.readline, ''):\n",
    "    print(line.strip())\n",
    "    \n",
    "    # Look for the trycloudflare.com URL\n",
    "    if 'trycloudflare.com' in line:\n",
    "        match = re.search(r'https://[\\w-]+\\.trycloudflare\\.com', line)\n",
    "        if match:\n",
    "            public_url = match.group(0)\n",
    "            break\n",
    "\n",
    "if public_url:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚úÖ TUNNEL ACTIVE!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nüåç Public URL: {public_url}\")\n",
    "    print(f\"\\nüìù API Endpoint: {public_url}/api\")\n",
    "    print(f\"\\nüîó Use this URL to connect to your Ollama instance from anywhere!\")\n",
    "    print(\"\\n‚ö†Ô∏è  Keep this cell running to maintain the tunnel.\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Display clickable link\n",
    "    display(HTML(f'<h2>Your Public Ollama URL:</h2><a href=\"{public_url}\" target=\"_blank\" style=\"font-size:20px; color:blue;\">{public_url}</a>'))\n",
    "    \n",
    "    # Store URL for later use\n",
    "    with open('/tmp/ollama_url.txt', 'w') as f:\n",
    "        f.write(public_url)\n",
    "else:\n",
    "    print(\"\\n‚ùå Failed to get public URL from cloudflared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section6"
   },
   "source": [
    "## 6. Download Models\n",
    "\n",
    "Choose which model to download. Available options include:\n",
    "- `llama3:8b` - Meta's Llama 3 8B model (~4.7GB)\n",
    "- `llama3:13b` - Meta's Llama 3 13B model (~7.4GB)\n",
    "- `mistral:7b` - Mistral 7B model (~4.1GB)\n",
    "- `codellama:13b` - Code Llama 13B (~7.4GB)\n",
    "- `qwen2.5:14b` - Qwen 2.5 14B model (~9GB)\n",
    "\n",
    "**Note:** Models are large! Download time depends on your connection speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_model"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Configure which model to download\n",
    "# Change this to your preferred model\n",
    "MODEL_NAME = \"llama3:13b\"  # You can change this to other models\n",
    "\n",
    "print(f\"üì• Downloading model: {MODEL_NAME}\")\n",
    "print(\"=\" * 80)\n",
    "print(\"‚è≥ This may take 10-30 minutes depending on the model size...\")\n",
    "print(\"\")\n",
    "\n",
    "try:\n",
    "    # Pull the model\n",
    "    process = subprocess.Popen(\n",
    "        ['ollama', 'pull', MODEL_NAME],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        text=True,\n",
    "        bufsize=1\n",
    "    )\n",
    "    \n",
    "    # Stream output\n",
    "    for line in iter(process.stdout.readline, ''):\n",
    "        if line:\n",
    "            print(line.strip())\n",
    "    \n",
    "    process.wait()\n",
    "    \n",
    "    if process.returncode == 0:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"‚úÖ Model '{MODEL_NAME}' downloaded successfully!\")\n",
    "        print(\"=\" * 80)\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Failed to download model '{MODEL_NAME}'\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error downloading model: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section7"
   },
   "source": [
    "## 7. List Available Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "list_models"
   },
   "outputs": [],
   "source": [
    "print(\"üìã Available Models:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "!ollama list\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section8"
   },
   "source": [
    "## 8. Test the Model Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_model"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def chat_with_ollama(prompt, model=\"llama3:13b\", stream=False):\n",
    "    \"\"\"\n",
    "    Send a chat request to the local Ollama instance\n",
    "    \"\"\"\n",
    "    url = \"http://localhost:11434/api/chat\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"stream\": stream\n",
    "    }\n",
    "    \n",
    "    print(f\"ü§ñ Asking {model}: {prompt}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Response:\")\n",
    "    print()\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=payload, stream=stream)\n",
    "        \n",
    "        if stream:\n",
    "            full_response = \"\"\n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    data = json.loads(line)\n",
    "                    if 'message' in data:\n",
    "                        content = data['message'].get('content', '')\n",
    "                        print(content, end='', flush=True)\n",
    "                        full_response += content\n",
    "            print()  # New line at the end\n",
    "            return full_response\n",
    "        else:\n",
    "            data = response.json()\n",
    "            if 'message' in data:\n",
    "                content = data['message'].get('content', '')\n",
    "                print(content)\n",
    "                return content\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    finally:\n",
    "        print()\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Test with a simple prompt\n",
    "chat_with_ollama(\n",
    "    \"Explain what Ollama is in 2 sentences.\",\n",
    "    model=MODEL_NAME,\n",
    "    stream=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section9"
   },
   "source": [
    "## 9. Test Public URL Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_public"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Read the public URL\n",
    "try:\n",
    "    with open('/tmp/ollama_url.txt', 'r') as f:\n",
    "        public_url = f.read().strip()\n",
    "    \n",
    "    print(f\"üåê Testing public access via: {public_url}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Test the /api/tags endpoint\n",
    "    response = requests.get(f\"{public_url}/api/tags\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(\"‚úÖ Public URL is accessible!\")\n",
    "        print(\"\\nAvailable models via public URL:\")\n",
    "        data = response.json()\n",
    "        if 'models' in data:\n",
    "            for model in data['models']:\n",
    "                print(f\"  - {model.get('name', 'Unknown')}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to access public URL (Status: {response.status_code})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üìù Example API Usage:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\"\"\n",
    "# Python example:\n",
    "import requests\n",
    "\n",
    "url = \"{public_url}/api/chat\"\n",
    "payload = {{\n",
    "    \"model\": \"{MODEL_NAME}\",\n",
    "    \"messages\": [\n",
    "        {{\"role\": \"user\", \"content\": \"Hello!\"}}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "print(response.json())\n",
    "\n",
    "# cURL example:\n",
    "curl {public_url}/api/chat -d '{{\n",
    "  \"model\": \"{MODEL_NAME}\",\n",
    "  \"messages\": [\n",
    "    {{\"role\": \"user\", \"content\": \"Hello!\"}}\n",
    "  ]\n",
    "}}'\n",
    "    \"\"\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Public URL not found. Make sure the Cloudflare tunnel is running.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error testing public URL: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section10"
   },
   "source": [
    "## 10. Interactive Chat Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "interactive_chat"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def interactive_chat(model=MODEL_NAME, use_public_url=False):\n",
    "    \"\"\"\n",
    "    Simple interactive chat interface\n",
    "    \"\"\"\n",
    "    # Determine which URL to use\n",
    "    if use_public_url:\n",
    "        try:\n",
    "            with open('/tmp/ollama_url.txt', 'r') as f:\n",
    "                base_url = f.read().strip()\n",
    "        except:\n",
    "            print(\"‚ùå Public URL not available, using localhost\")\n",
    "            base_url = \"http://localhost:11434\"\n",
    "    else:\n",
    "        base_url = \"http://localhost:11434\"\n",
    "    \n",
    "    url = f\"{base_url}/api/chat\"\n",
    "    conversation_history = []\n",
    "    \n",
    "    print(\"ü§ñ Interactive Chat with Ollama\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Model: {model}\")\n",
    "    print(f\"Endpoint: {base_url}\")\n",
    "    print(\"\\nType your message and press Enter. Type 'quit' to exit.\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_input = input(\"You: \").strip()\n",
    "        \n",
    "        if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "            print(\"\\nüëã Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        # Add user message to history\n",
    "        conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        # Send request\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": conversation_history,\n",
    "            \"stream\": True\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            print(\"\\nAssistant: \", end=\"\", flush=True)\n",
    "            response = requests.post(url, json=payload, stream=True)\n",
    "            \n",
    "            full_response = \"\"\n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    data = json.loads(line)\n",
    "                    if 'message' in data:\n",
    "                        content = data['message'].get('content', '')\n",
    "                        print(content, end='', flush=True)\n",
    "                        full_response += content\n",
    "            \n",
    "            print()  # New line\n",
    "            print()\n",
    "            \n",
    "            # Add assistant response to history\n",
    "            conversation_history.append({\"role\": \"assistant\", \"content\": full_response})\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error: {str(e)}\")\n",
    "            print()\n",
    "\n",
    "# Start interactive chat\n",
    "interactive_chat(model=MODEL_NAME, use_public_url=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section11"
   },
   "source": [
    "## 11. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "utilities"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def show_status():\n",
    "    \"\"\"Display current status of Ollama and tunnel\"\"\"\n",
    "    print(\"üìä Status Report\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Check Ollama server\n",
    "    try:\n",
    "        response = requests.get('http://localhost:11434/api/tags', timeout=2)\n",
    "        if response.status_code == 200:\n",
    "            print(\"‚úÖ Ollama server: RUNNING\")\n",
    "            data = response.json()\n",
    "            models = data.get('models', [])\n",
    "            print(f\"   Models loaded: {len(models)}\")\n",
    "        else:\n",
    "            print(\"‚ùå Ollama server: NOT RESPONDING\")\n",
    "    except:\n",
    "        print(\"‚ùå Ollama server: OFFLINE\")\n",
    "    \n",
    "    # Check public URL\n",
    "    try:\n",
    "        with open('/tmp/ollama_url.txt', 'r') as f:\n",
    "            public_url = f.read().strip()\n",
    "        print(f\"‚úÖ Public URL: {public_url}\")\n",
    "        \n",
    "        # Test public access\n",
    "        response = requests.get(f\"{public_url}/api/tags\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(\"   Status: ACCESSIBLE\")\n",
    "        else:\n",
    "            print(\"   Status: NOT ACCESSIBLE\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Public URL: NOT CONFIGURED\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Public URL: ERROR ({str(e)})\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "\n",
    "def download_additional_model(model_name):\n",
    "    \"\"\"Download an additional model\"\"\"\n",
    "    print(f\"üì• Downloading {model_name}...\")\n",
    "    !ollama pull {model_name}\n",
    "    print(f\"\\n‚úÖ Model {model_name} downloaded!\")\n",
    "\n",
    "def get_public_url():\n",
    "    \"\"\"Get the current public URL\"\"\"\n",
    "    try:\n",
    "        with open('/tmp/ollama_url.txt', 'r') as f:\n",
    "            return f.read().strip()\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Run status check\n",
    "show_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section12"
   },
   "source": [
    "## 12. Keep-Alive Cell\n",
    "\n",
    "Run this cell to keep both Ollama and the Cloudflare tunnel running.\n",
    "This will prevent the notebook from timing out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "keep_alive"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üîÑ Keep-Alive Monitor\")\n",
    "print(\"=\" * 80)\n",
    "print(\"This cell will run indefinitely to keep services active.\")\n",
    "print(\"Press the Stop button to interrupt.\\n\")\n",
    "\n",
    "try:\n",
    "    counter = 0\n",
    "    while True:\n",
    "        counter += 1\n",
    "        current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        # Check Ollama\n",
    "        try:\n",
    "            requests.get('http://localhost:11434/api/tags', timeout=2)\n",
    "            ollama_status = \"‚úÖ\"\n",
    "        except:\n",
    "            ollama_status = \"‚ùå\"\n",
    "        \n",
    "        # Check tunnel\n",
    "        try:\n",
    "            with open('/tmp/ollama_url.txt', 'r') as f:\n",
    "                public_url = f.read().strip()\n",
    "            requests.get(f\"{public_url}/api/tags\", timeout=5)\n",
    "            tunnel_status = \"‚úÖ\"\n",
    "        except:\n",
    "            tunnel_status = \"‚ùå\"\n",
    "        \n",
    "        print(f\"[{current_time}] Check #{counter} - Ollama: {ollama_status} | Tunnel: {tunnel_status}\", end='\\r')\n",
    "        \n",
    "        time.sleep(60)  # Check every minute\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\n‚èπÔ∏è  Keep-alive monitor stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section13"
   },
   "source": [
    "## üìö Additional Information\n",
    "\n",
    "### Available Models\n",
    "\n",
    "You can download additional models using:\n",
    "```python\n",
    "!ollama pull <model-name>\n",
    "```\n",
    "\n",
    "Popular models:\n",
    "- `llama3:8b` - Llama 3 8B\n",
    "- `llama3:13b` - Llama 3 13B\n",
    "- `llama3:70b` - Llama 3 70B (requires significant resources)\n",
    "- `mistral:7b` - Mistral 7B\n",
    "- `codellama:13b` - Code Llama 13B\n",
    "- `qwen2.5:14b` - Qwen 2.5 14B\n",
    "\n",
    "### API Endpoints\n",
    "\n",
    "- **Chat**: `POST /api/chat`\n",
    "- **Generate**: `POST /api/generate`\n",
    "- **List Models**: `GET /api/tags`\n",
    "- **Pull Model**: `POST /api/pull`\n",
    "- **Delete Model**: `DELETE /api/delete`\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "1. **Resource Usage**: Large models require significant RAM and GPU memory\n",
    "2. **Session Limits**: Free Colab sessions have time limits (~12 hours)\n",
    "3. **Tunnel Duration**: The Cloudflare tunnel URL changes each time you restart\n",
    "4. **Security**: The public URL is accessible to anyone - avoid sharing sensitive data\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "- **Out of Memory**: Try using smaller models (7B or 8B instead of 13B+)\n",
    "- **Slow Performance**: Ensure GPU is enabled in Colab settings\n",
    "- **Tunnel Issues**: Restart the Cloudflare tunnel cell\n",
    "- **Model Download Fails**: Check internet connection and disk space\n",
    "\n",
    "### Documentation\n",
    "\n",
    "- Ollama: https://github.com/ollama/ollama\n",
    "- Ollama API: https://github.com/ollama/ollama/blob/main/docs/api.md\n",
    "- Cloudflare Tunnel: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
